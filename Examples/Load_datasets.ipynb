{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining MI_prediction from git+https://github.com/UN-GCPDS/python-gcpds.MI_prediction.git#egg=MI_prediction\n",
      "  Updating ./src/mi-prediction clone\n",
      "  Running command git fetch -q --tags\n",
      "  Running command git reset --hard -q 28608a7026ff01c811c40d666bbe3b0b7b2b3ed1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: moabb in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from MI_prediction) (0.4.6)\n",
      "Requirement already satisfied: braindecode in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from MI_prediction) (0.6)\n",
      "Requirement already satisfied: h5py in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (3.7.0)\n",
      "Requirement already satisfied: mne in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (1.0.3)\n",
      "Requirement already satisfied: pandas in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (1.4.3)\n",
      "Requirement already satisfied: skorch in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (0.11.0)\n",
      "Requirement already satisfied: matplotlib in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (3.5.2)\n",
      "Requirement already satisfied: scipy in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (1.8.1)\n",
      "Requirement already satisfied: numpy in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (1.23.1)\n",
      "Requirement already satisfied: pooch<2.0,>=1.6 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (1.6.0)\n",
      "Requirement already satisfied: PyYAML<6.0,>=5.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (5.4.1)\n",
      "Requirement already satisfied: scikit-learn<2.0,>=1.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (1.1.1)\n",
      "Requirement already satisfied: coverage<6.0,>=5.5 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (5.5)\n",
      "Requirement already satisfied: pyriemann>=0.2.6 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (0.3)\n",
      "Requirement already satisfied: seaborn>=0.9 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (0.11.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.62 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.15.1 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (2.28.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (4.34.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (9.2.0)\n",
      "Requirement already satisfied: decorator in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from mne->braindecode->MI_prediction) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from mne->braindecode->MI_prediction) (3.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from pandas->braindecode->MI_prediction) (2022.1)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from pooch<2.0,>=1.6->moabb->MI_prediction) (1.4.4)\n",
      "Requirement already satisfied: joblib in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from pyriemann>=0.2.6->moabb->MI_prediction) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from requests<3.0.0,>=2.15.1->moabb->MI_prediction) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from requests<3.0.0,>=2.15.1->moabb->MI_prediction) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from requests<3.0.0,>=2.15.1->moabb->MI_prediction) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from requests<3.0.0,>=2.15.1->moabb->MI_prediction) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from scikit-learn<2.0,>=1.0->moabb->MI_prediction) (3.1.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from skorch->braindecode->MI_prediction) (0.8.10)\n",
      "Requirement already satisfied: six>=1.5 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->braindecode->MI_prediction) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from jinja2->mne->braindecode->MI_prediction) (2.1.1)\n",
      "Installing collected packages: MI_prediction\n",
      "  Attempting uninstall: MI_prediction\n",
      "    Found existing installation: MI-prediction 0.1\n",
      "    Uninstalling MI-prediction-0.1:\n",
      "      Successfully uninstalled MI-prediction-0.1\n",
      "  Running setup.py develop for MI_prediction\n",
      "Successfully installed MI_prediction-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e git+https://github.com/UN-GCPDS/python-gcpds.MI_prediction.git#egg=MI_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cho2017 resting state class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from mne import create_info\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.io import RawArray\n",
    "from scipy.io import loadmat\n",
    "from moabb.datasets import download as dl\n",
    "from moabb.datasets.base import BaseDataset\n",
    "\n",
    "from braindecode.preprocessing.preprocess import exponential_moving_standardize, preprocess, Preprocessor, scale\n",
    "from braindecode.datasets import create_from_mne_raw\n",
    "import pandas as pd\n",
    "from braindecode.datasets import BaseDataset as BD\n",
    "from braindecode.datasets import BaseConcatDataset\n",
    "from braindecode.preprocessing.windowers import create_windows_from_events,_create_fixed_length_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "GIGA_URL = \"ftp://parrot.genomics.cn/gigadb/pub/10.5524/100001_101000/100295/mat_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cho2017_Rest(BaseDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            subjects=list(range(1, 53)),\n",
    "            sessions_per_subject=1,\n",
    "            events=dict(rest=1),\n",
    "            code=\"Cho2017_Rest\",\n",
    "            interval=[0, 60],  # full trial is 0-3s, but edge effects\n",
    "            paradigm=\"imagery\",\n",
    "            doi=\"10.5524/100295\",\n",
    "        )\n",
    "\n",
    "        for ii in [32, 46, 49]:\n",
    "            self.subject_list.remove(ii)\n",
    "\n",
    "    def _get_single_subject_data(self, subject):\n",
    "        \"\"\"return data for a single subject\"\"\"\n",
    "        fname = self.data_path(subject)\n",
    "\n",
    "        data = loadmat(\n",
    "            fname,\n",
    "            squeeze_me=True,\n",
    "            struct_as_record=False,\n",
    "            verify_compressed_data_integrity=False,\n",
    "        )[\"eeg\"]\n",
    "\n",
    "        # fmt: off\n",
    "        eeg_ch_names = [\n",
    "            \"Fp1\", \"AF7\", \"AF3\", \"F1\", \"F3\", \"F5\", \"F7\", \"FT7\", \"FC5\", \"FC3\", \"FC1\",\n",
    "            \"C1\", \"C3\", \"C5\", \"T7\", \"TP7\", \"CP5\", \"CP3\", \"CP1\", \"P1\", \"P3\", \"P5\", \"P7\",\n",
    "            \"P9\", \"PO7\", \"PO3\", \"O1\", \"Iz\", \"Oz\", \"POz\", \"Pz\", \"CPz\", \"Fpz\", \"Fp2\",\n",
    "            \"AF8\", \"AF4\", \"AFz\", \"Fz\", \"F2\", \"F4\", \"F6\", \"F8\", \"FT8\", \"FC6\", \"FC4\",\n",
    "            \"FC2\", \"FCz\", \"Cz\", \"C2\", \"C4\", \"C6\", \"T8\", \"TP8\", \"CP6\", \"CP4\", \"CP2\",\n",
    "            \"P2\", \"P4\", \"P6\", \"P8\", \"P10\", \"PO8\", \"PO4\", \"O2\",\n",
    "        ]\n",
    "        # fmt: on\n",
    "        emg_ch_names = [\"EMG1\", \"EMG2\", \"EMG3\", \"EMG4\"]\n",
    "        ch_names = eeg_ch_names + emg_ch_names \n",
    "        ch_types = [\"eeg\"] * 64 + [\"emg\"] * 4 \n",
    "        montage = make_standard_montage(\"standard_1005\")\n",
    "        resting = data.rest - data.rest.mean(axis=1, keepdims=True)\n",
    "        \n",
    "        eeg_rest = resting * 1e-6\n",
    "\n",
    "        info = create_info(ch_names=ch_names, ch_types=ch_types, sfreq=data.srate)\n",
    "        raw = RawArray(data=eeg_rest, info=info, verbose=False)\n",
    "        raw.set_montage(montage)\n",
    "\n",
    "        return {\"session_0\": {\"run_0\": raw}}\n",
    "\n",
    "    def data_path(\n",
    "        self, subject, path=None, force_update=False, update_path=None, verbose=None\n",
    "    ):\n",
    "        if subject not in self.subject_list:\n",
    "            raise (ValueError(\"Invalid subject number\"))\n",
    "\n",
    "        url = \"{:s}s{:02d}.mat\".format(GIGA_URL, subject)\n",
    "        return dl.data_dl(url, \"GIGADB\", path, force_update, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_r = Cho2017_Rest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = ds_r.get_data([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'session_0': {'run_0': <RawArray | 68 x 34048 (66.5 s), ~17.8 MB, data loaded>}}}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fetch_and_unpack_moabb_data(dataset, subject_ids):\n",
    "    data = dataset.get_data(subject_ids)\n",
    "    raws, subject_ids, session_ids, run_ids = [], [], [], []\n",
    "    for subj_id, subj_data in data.items():\n",
    "        for sess_id, sess_data in subj_data.items():\n",
    "            for run_id, raw in sess_data.items():\n",
    "                raws.append(raw)\n",
    "                subject_ids.append(subj_id)\n",
    "                session_ids.append(sess_id)\n",
    "                run_ids.append(run_id)\n",
    "    description = pd.DataFrame({\n",
    "        'subject': subject_ids,\n",
    "        'session': session_ids,\n",
    "        'run': run_ids\n",
    "    })\n",
    "    return raws, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws,description = _fetch_and_unpack_moabb_data(ds_r, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<RawArray | 68 x 34048 (66.5 s), ~17.8 MB, data loaded>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, subject            1\n",
      "session    session_0\n",
      "run            run_0\n",
      "Name: 0, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "for i in description.iterrows():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_base_ds = [BD(raw, row)\n",
    "                       for raw, (_, row) in zip(raws, description.iterrows())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw, (_, row) in zip(raws, description.iterrows()):\n",
    "    db = BD(raw, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.base.BaseDataset at 0x299101880>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<braindecode.datasets.base.BaseDataset at 0x299e19970>]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_base_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOABBDataset_Rest(BaseConcatDataset):\n",
    "    def __init__(self, dataset, subject_ids, dataset_kwargs=None):\n",
    "        raws, description = _fetch_and_unpack_moabb_data(dataset, subject_ids)\n",
    "        all_base_ds = [BD(raw, row)\n",
    "                       for raw, (_, row) in zip(raws, description.iterrows())]\n",
    "        super().__init__(all_base_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MOABBDataset_Rest(dataset=Cho2017_Rest(), subject_ids=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.base.BaseDataset at 0x2b7f79d90>"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne import Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfreq = dataset.datasets[0].raw.info[\"sfreq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Annotations | 0 segments>"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.datasets[0].raw.annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.preprocessing.windowers import _compute_window_inds, _check_windowing_arguments, WindowsDataset\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset.datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_windows_from_events(\n",
    "        ds, infer_window_size_stride,\n",
    "        trial_start_offset_samples, trial_stop_offset_samples,\n",
    "        window_size_samples=None, window_stride_samples=None,\n",
    "        drop_last_window=False, preload=False,\n",
    "        drop_bad_windows=True, picks=None, reject=None, flat=None,\n",
    "        on_missing='error', accepted_bads_ratio=0.0):\n",
    "\n",
    "    #trial_start_offset_samples = 0 #\n",
    "    #trial_stop_offset_samples = 0 #\n",
    "    #drop_last_window = False #\n",
    "    #accepted_bads_ratio=0.0 #\n",
    "    events_id = None #\n",
    "    #drop_bad_windows = False #\n",
    "\n",
    "    duration = int(ds.raw.n_times/ds.raw.info[\"sfreq\"])\n",
    "    onsets = np.array([0])\n",
    "    stops = onsets+np.array([int(duration*ds.raw.info[\"sfreq\"])])\n",
    "\n",
    "    last_samp = ds.raw.first_samp + ds.raw.n_times\n",
    "    if stops[-1] + trial_stop_offset_samples > last_samp:\n",
    "        raise ValueError(\n",
    "            '\"trial_stop_offset_samples\" too large. Stop of last trial '\n",
    "            f'({stops[-1]}) + \"trial_stop_offset_samples\" '\n",
    "            f'({trial_stop_offset_samples}) must be smaller than length of'\n",
    "            f' recording ({len(ds)}).')\n",
    "\n",
    "    window_size_samples = stops[0] + trial_stop_offset_samples - (onsets[0] + trial_start_offset_samples)\n",
    "    window_stride_samples = window_size_samples\n",
    "\n",
    "    i_trials, i_window_in_trials, starts, stops = _compute_window_inds(onsets, stops, trial_start_offset_samples,\n",
    "        trial_stop_offset_samples, window_size_samples, window_stride_samples, drop_last_window,\n",
    "        accepted_bads_ratio)\n",
    "        \n",
    "    description = -1\n",
    "    events = [[start, window_size_samples, description]\n",
    "                for i_start, start in enumerate(starts)]\n",
    "\n",
    "    events = np.array(events)\n",
    "\n",
    "    description = events[:, -1]\n",
    "\n",
    "    metadata = pd.DataFrame({\n",
    "        'i_window_in_trial': i_window_in_trials,\n",
    "        'i_start_in_trial': starts,\n",
    "        'i_stop_in_trial': stops,\n",
    "        'target': description})\n",
    "\n",
    "    mne_epochs = mne.Epochs(\n",
    "        dataset.datasets[0].raw, events, events_id, baseline=None, tmin=0,\n",
    "        tmax=(window_size_samples - 1) / ds.raw.info[\"sfreq\"],\n",
    "        metadata=metadata,)\n",
    "\n",
    "    if drop_bad_windows:\n",
    "            mne_epochs.drop_bad()\n",
    "\n",
    "    windows_ds = WindowsDataset(mne_epochs, ds.description)\n",
    "    return windows_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows_from_events(\n",
    "        concat_ds, trial_start_offset_samples=0, trial_stop_offset_samples=0,\n",
    "        window_size_samples=None, window_stride_samples=None,\n",
    "        drop_last_window=False, preload=False,\n",
    "        drop_bad_windows=True, picks=None, reject=None, flat=None,\n",
    "        on_missing='error', accepted_bads_ratio=0.0, n_jobs=1):\n",
    "\n",
    "    _check_windowing_arguments(\n",
    "        trial_start_offset_samples, trial_stop_offset_samples,\n",
    "        window_size_samples, window_stride_samples)\n",
    "\n",
    "    infer_window_size_stride = window_size_samples is None\n",
    "\n",
    "    list_of_windows_ds = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(_create_windows_from_events)(\n",
    "            ds, infer_window_size_stride,\n",
    "            trial_start_offset_samples, trial_stop_offset_samples,\n",
    "            window_size_samples, window_stride_samples, drop_last_window,\n",
    "            preload, drop_bad_windows, picks, reject, flat,\n",
    "            on_missing, accepted_bads_ratio) for ds in concat_ds.datasets)\n",
    "\n",
    "    return BaseConcatDataset(list_of_windows_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 4 columns\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1 events and 33792 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "trials = create_windows_from_events(dataset, trial_start_offset_samples=0, trial_stop_offset_samples=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Cho2017 resting state class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MI_prediction.Datasets.Moabb import MOABBDataset_Rest\n",
    "from MI_prediction.Datasets import Cho2017_Rest\n",
    "from MI_prediction.Utils.Windowers import create_windows_from_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MOABBDataset_Rest(dataset=Cho2017_Rest(), subject_ids=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 4 columns\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1 events and 33792 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "sfreq = dataset.datasets[0].raw.info[\"sfreq\"]\n",
    "trials= create_windows_from_events(dataset,trial_start_offset_samples=int(0*sfreq),\n",
    "                    trial_stop_offset_samples=int(0*sfreq), preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 33792)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create BCI IIa resting state class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from moabb.datasets.bnci import data_path\n",
    "from moabb.datasets.bnci import _convert_run as crun\n",
    "from moabb.datasets.base import BaseDataset\n",
    "from scipy.io import loadmat\n",
    "from mne.channels import make_standard_montage\n",
    "from mne import create_info\n",
    "from mne.io import RawArray\n",
    "from mne.utils import verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names = [\n",
    "        \"Fz\", \"FC3\", \"FC1\", \"FCz\", \"FC2\", \"FC4\", \"C5\", \"C3\", \"C1\", \"Cz\", \"C2\",\n",
    "        \"C4\", \"C6\", \"CP3\", \"CP1\", \"CPz\", \"CP2\", \"CP4\", \"P1\", \"Pz\", \"P2\", \"POz\",\n",
    "        \"EOG1\", \"EOG2\", \"EOG3\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_types = [\"eeg\"] * 22 + [\"eog\"] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_load_data_001_2014\n",
    "BNCI_URL = \"http://bnci-horizon-2020.eu/database/data-sets/\"\n",
    "base_url=BNCI_URL #\n",
    "subject = 1\n",
    "r = 'T'\n",
    "path = None\n",
    "force_update = False\n",
    "update_path = None\n",
    "\n",
    "url = \"{u}001-2014/A{s:02d}{r}.mat\".format(u=base_url, s=subject, r=r)\n",
    "filename = data_path(url, path, force_update, update_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_convert_mi\n",
    "filename = filename[0] #\n",
    "\n",
    "runs = []\n",
    "event_id = {}\n",
    "data = loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
    "\n",
    "if isinstance(data[\"data\"], np.ndarray):\n",
    "    run_array = data[\"data\"]\n",
    "else:\n",
    "    run_array = [data[\"data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_convert_run\n",
    "run = run_array[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_convert_run\n",
    "raw, evd = crun(run, ch_names, ch_types, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@verbose\n",
    "def _convert_run(run, ch_names=None, ch_types=None, verbose=None):\n",
    "    \"\"\"Convert one run to raw.\"\"\"\n",
    "    # parse eeg data\n",
    "    event_id = {}\n",
    "    n_chan = run.X.shape[1]\n",
    "    montage = make_standard_montage(\"standard_1005\")\n",
    "    eeg_data = 1e-6 * run.X\n",
    "    sfreq = run.fs\n",
    "\n",
    "    if not ch_names:\n",
    "        ch_names = [\"EEG%d\" % ch for ch in range(1, n_chan + 1)]\n",
    "        montage = None  # no montage\n",
    "\n",
    "    if not ch_types:\n",
    "        ch_types = [\"eeg\"] * n_chan\n",
    "\n",
    "    info = create_info(ch_names=ch_names, ch_types=ch_types, sfreq=sfreq)\n",
    "    raw = RawArray(data=eeg_data.T, info=info, verbose=verbose)\n",
    "    raw.set_montage(montage)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=25, n_times=41463\n",
      "    Range : 0 ... 41462 =      0.000 ...   165.848 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "raw = _convert_run(run, ch_names, ch_types, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_mi(filename, ch_names, ch_types):\n",
    "    runs = []\n",
    "    event_id = {}\n",
    "    data = loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
    "\n",
    "    if isinstance(data[\"data\"], np.ndarray):\n",
    "        run_array = data[\"data\"]\n",
    "    else:\n",
    "        run_array = [data[\"data\"]]\n",
    "\n",
    "    for run in run_array:\n",
    "        if len(run.y)==0:\n",
    "            raw = _convert_run(run, ch_names, ch_types, None)\n",
    "            runs.append(raw)\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/julian/mne_data/MNE-bnci-data/database/data-sets/001-2014/A01T.mat'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=25, n_times=29683\n",
      "    Range : 0 ... 29682 =      0.000 ...   118.728 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=25, n_times=20172\n",
      "    Range : 0 ... 20171 =      0.000 ...    80.684 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=25, n_times=41463\n",
      "    Range : 0 ... 41462 =      0.000 ...   165.848 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "runs = _convert_mi(filename, ch_names, ch_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<RawArray | 25 x 29683 (118.7 s), ~5.7 MB, data loaded>,\n",
       " <RawArray | 25 x 20172 (80.7 s), ~3.9 MB, data loaded>,\n",
       " <RawArray | 25 x 41463 (165.8 s), ~7.9 MB, data loaded>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@verbose\n",
    "def _load_data_001_2014(\n",
    "    subject,\n",
    "    path=None,\n",
    "    force_update=False,\n",
    "    update_path=None,\n",
    "    base_url=BNCI_URL,\n",
    "    verbose=None,\n",
    "):\n",
    "    \"\"\"Load data for 001-2014 dataset.\"\"\"\n",
    "    if (subject < 1) or (subject > 9):\n",
    "        raise ValueError(\"Subject must be between 1 and 9. Got %d.\" % subject)\n",
    "\n",
    "    # fmt: off\n",
    "    ch_names = [\n",
    "        \"Fz\", \"FC3\", \"FC1\", \"FCz\", \"FC2\", \"FC4\", \"C5\", \"C3\", \"C1\", \"Cz\", \"C2\",\n",
    "        \"C4\", \"C6\", \"CP3\", \"CP1\", \"CPz\", \"CP2\", \"CP4\", \"P1\", \"Pz\", \"P2\", \"POz\",\n",
    "        \"EOG1\", \"EOG2\", \"EOG3\",\n",
    "    ]\n",
    "    # fmt: on\n",
    "    ch_types = [\"eeg\"] * 22 + [\"eog\"] * 3\n",
    "\n",
    "    sessions = {}\n",
    "    for r in [\"T\", \"E\"]:\n",
    "        url = \"{u}001-2014/A{s:02d}{r}.mat\".format(u=base_url, s=subject, r=r)\n",
    "        filename = data_path(url, path, force_update, update_path)\n",
    "        runs = _convert_mi(filename[0], ch_names, ch_types)\n",
    "        # FIXME: deal with run with no event (1:3) and name them\n",
    "        sessions[\"session_%s\" % r] = {\"run_%d\" % ii: run for ii, run in enumerate(runs)}\n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject,\n",
    "dataset=_load_data_001_2014\n",
    "path=None\n",
    "force_update=False\n",
    "update_path=None\n",
    "base_url=BNCI_URL\n",
    "verbose=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(\n",
    "    subject,\n",
    "    dataset=\"001-2014\",\n",
    "    path=None,\n",
    "    force_update=False,\n",
    "    update_path=None,\n",
    "    base_url=BNCI_URL,\n",
    "    verbose=None):  # noqa: D301\n",
    "    \n",
    "    dataset_list = {\n",
    "        \"001-2014\": _load_data_001_2014,\n",
    "    }\n",
    "\n",
    "    baseurl_list = {\n",
    "        \"001-2014\": BNCI_URL,}\n",
    "\n",
    "    if dataset not in dataset_list.keys():\n",
    "        raise ValueError(\n",
    "            \"Dataset '%s' is not a valid BNCI dataset ID. \"\n",
    "            \"Valid dataset are %s.\" % (dataset, \", \".join(dataset_list.keys()))\n",
    "        )\n",
    "\n",
    "    return dataset_list[dataset](\n",
    "        subject, path, force_update, update_path, baseurl_list[dataset], verbose\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.75"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "165/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNEBNCI(BaseDataset):\n",
    "    \"\"\"Base BNCI dataset\"\"\"\n",
    "\n",
    "    def _get_single_subject_data(self, subject):\n",
    "        \"\"\"return data for a single subject\"\"\"\n",
    "        sessions = load_data(subject=subject, dataset=self.code, verbose=False)\n",
    "        return sessions\n",
    "\n",
    "    def data_path(\n",
    "        self, subject, path=None, force_update=False, update_path=None, verbose=None\n",
    "    ):\n",
    "        return load_data(\n",
    "            subject=subject,\n",
    "            dataset=self.code,\n",
    "            verbose=verbose,\n",
    "            update_path=update_path,\n",
    "            path=path,\n",
    "            force_update=force_update,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNCI2014001(MNEBNCI):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            subjects=list(range(1, 10)),\n",
    "            sessions_per_subject=2,\n",
    "            events={\"rest\": 1},\n",
    "            code=\"001-2014\",\n",
    "            interval=[0, 2],\n",
    "            paradigm=\"imagery\",\n",
    "            doi=\"10.3389/fnins.2012.00055\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = BNCI2014001()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'session_T': {'run_0': <RawArray | 25 x 29683 (118.7 s), ~5.7 MB, data loaded>,\n",
       "   'run_1': <RawArray | 25 x 20172 (80.7 s), ~3.9 MB, data loaded>,\n",
       "   'run_2': <RawArray | 25 x 41463 (165.8 s), ~7.9 MB, data loaded>},\n",
       "  'session_E': {'run_0': <RawArray | 25 x 34291 (137.2 s), ~6.6 MB, data loaded>,\n",
       "   'run_1': <RawArray | 25 x 34459 (137.8 s), ~6.6 MB, data loaded>,\n",
       "   'run_2': <RawArray | 25 x 37040 (148.2 s), ~7.1 MB, data loaded>}}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.get_data([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test BCI IIa resting state class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MI_prediction.Datasets.Moabb import MOABBDataset_Rest\n",
    "from MI_prediction.Datasets import BNCI2014001_Rest\n",
    "from MI_prediction.Utils.Windowers import create_windows_from_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MOABBDataset_Rest(dataset=BNCI2014001_Rest(), subject_ids=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 4 columns\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1 events and 29500 original time points ...\n",
      "0 bad epochs dropped\n",
      "Adding metadata with 4 columns\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1 events and 20000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Adding metadata with 4 columns\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1 events and 41250 original time points ...\n",
      "0 bad epochs dropped\n",
      "Adding metadata with 4 columns\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1 events and 34250 original time points ...\n",
      "0 bad epochs dropped\n",
      "Adding metadata with 4 columns\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1 events and 34250 original time points ...\n",
      "0 bad epochs dropped\n",
      "Adding metadata with 4 columns\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1 events and 37000 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "sfreq = dataset.datasets[0].raw.info[\"sfreq\"]\n",
    "trials= create_windows_from_events(dataset,trial_start_offset_samples=int(0*sfreq),\n",
    "                    trial_stop_offset_samples=int(0*sfreq), preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trials[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>session</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>session_T</td>\n",
       "      <td>run_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>session_T</td>\n",
       "      <td>run_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>session_T</td>\n",
       "      <td>run_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>session_E</td>\n",
       "      <td>run_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>session_E</td>\n",
       "      <td>run_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>session_E</td>\n",
       "      <td>run_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject    session    run\n",
       "0        1  session_T  run_0\n",
       "1        1  session_T  run_1\n",
       "2        1  session_T  run_2\n",
       "3        1  session_E  run_0\n",
       "4        1  session_E  run_1\n",
       "5        1  session_E  run_2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.description"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('MI_pred')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6629aad805afa7a6a59ced7c9bb143c65b3c10657ba34c3536df57a876399260"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
