{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining MI_prediction from git+https://github.com/UN-GCPDS/python-gcpds.MI_prediction.git#egg=MI_prediction\n",
      "  Updating ./src/mi-prediction clone\n",
      "  Running command git fetch -q --tags\n",
      "  Running command git reset --hard -q 02f0cf71c60b1ca389163f6a338a97bf25a445a6\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: moabb in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from MI_prediction) (0.4.6)\n",
      "Requirement already satisfied: braindecode in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from MI_prediction) (0.6)\n",
      "Requirement already satisfied: skorch in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (0.11.0)\n",
      "Requirement already satisfied: matplotlib in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (3.5.2)\n",
      "Requirement already satisfied: numpy in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (1.23.1)\n",
      "Requirement already satisfied: h5py in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (3.7.0)\n",
      "Requirement already satisfied: pandas in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (1.4.3)\n",
      "Requirement already satisfied: scipy in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (1.8.1)\n",
      "Requirement already satisfied: mne in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from braindecode->MI_prediction) (1.0.3)\n",
      "Requirement already satisfied: pooch<2.0,>=1.6 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (1.6.0)\n",
      "Requirement already satisfied: scikit-learn<2.0,>=1.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (1.1.1)\n",
      "Requirement already satisfied: seaborn>=0.9 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (0.11.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.62 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (4.64.0)\n",
      "Requirement already satisfied: PyYAML<6.0,>=5.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (5.4.1)\n",
      "Requirement already satisfied: pyriemann>=0.2.6 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.15.1 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (2.28.1)\n",
      "Requirement already satisfied: coverage<6.0,>=5.5 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from moabb->MI_prediction) (5.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (4.34.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from matplotlib->braindecode->MI_prediction) (21.3)\n",
      "Requirement already satisfied: decorator in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from mne->braindecode->MI_prediction) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from mne->braindecode->MI_prediction) (3.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from pandas->braindecode->MI_prediction) (2022.1)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from pooch<2.0,>=1.6->moabb->MI_prediction) (1.4.4)\n",
      "Requirement already satisfied: joblib in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from pyriemann>=0.2.6->moabb->MI_prediction) (1.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from requests<3.0.0,>=2.15.1->moabb->MI_prediction) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from requests<3.0.0,>=2.15.1->moabb->MI_prediction) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from requests<3.0.0,>=2.15.1->moabb->MI_prediction) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from requests<3.0.0,>=2.15.1->moabb->MI_prediction) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from scikit-learn<2.0,>=1.0->moabb->MI_prediction) (3.1.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from skorch->braindecode->MI_prediction) (0.8.10)\n",
      "Requirement already satisfied: six>=1.5 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->braindecode->MI_prediction) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/julian/miniforge3/envs/MI_pred/lib/python3.9/site-packages (from jinja2->mne->braindecode->MI_prediction) (2.1.1)\n",
      "Installing collected packages: MI_prediction\n",
      "  Attempting uninstall: MI_prediction\n",
      "    Found existing installation: MI-prediction 0.1\n",
      "    Uninstalling MI-prediction-0.1:\n",
      "      Successfully uninstalled MI-prediction-0.1\n",
      "  Running setup.py develop for MI_prediction\n",
      "Successfully installed MI_prediction-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e git+https://github.com/UN-GCPDS/python-gcpds.MI_prediction.git#egg=MI_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cho2017 resting state class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from mne import create_info\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.io import RawArray\n",
    "from scipy.io import loadmat\n",
    "from moabb.datasets import download as dl\n",
    "from moabb.datasets.base import BaseDataset\n",
    "\n",
    "from braindecode.preprocessing.preprocess import exponential_moving_standardize, preprocess, Preprocessor, scale\n",
    "from braindecode.datasets import create_from_mne_raw\n",
    "import pandas as pd\n",
    "from braindecode.datasets import BaseDataset as BD\n",
    "from braindecode.datasets import BaseConcatDataset\n",
    "from braindecode.preprocessing.windowers import create_windows_from_events,_create_fixed_length_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "GIGA_URL = \"ftp://parrot.genomics.cn/gigadb/pub/10.5524/100001_101000/100295/mat_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cho2017_Rest(BaseDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            subjects=list(range(1, 53)),\n",
    "            sessions_per_subject=1,\n",
    "            events=dict(rest=1),\n",
    "            code=\"Cho2017_Rest\",\n",
    "            interval=[0, 60],  # full trial is 0-3s, but edge effects\n",
    "            paradigm=\"imagery\",\n",
    "            doi=\"10.5524/100295\",\n",
    "        )\n",
    "\n",
    "        for ii in [32, 46, 49]:\n",
    "            self.subject_list.remove(ii)\n",
    "\n",
    "    def _get_single_subject_data(self, subject):\n",
    "        \"\"\"return data for a single subject\"\"\"\n",
    "        fname = self.data_path(subject)\n",
    "\n",
    "        data = loadmat(\n",
    "            fname,\n",
    "            squeeze_me=True,\n",
    "            struct_as_record=False,\n",
    "            verify_compressed_data_integrity=False,\n",
    "        )[\"eeg\"]\n",
    "\n",
    "        # fmt: off\n",
    "        eeg_ch_names = [\n",
    "            \"Fp1\", \"AF7\", \"AF3\", \"F1\", \"F3\", \"F5\", \"F7\", \"FT7\", \"FC5\", \"FC3\", \"FC1\",\n",
    "            \"C1\", \"C3\", \"C5\", \"T7\", \"TP7\", \"CP5\", \"CP3\", \"CP1\", \"P1\", \"P3\", \"P5\", \"P7\",\n",
    "            \"P9\", \"PO7\", \"PO3\", \"O1\", \"Iz\", \"Oz\", \"POz\", \"Pz\", \"CPz\", \"Fpz\", \"Fp2\",\n",
    "            \"AF8\", \"AF4\", \"AFz\", \"Fz\", \"F2\", \"F4\", \"F6\", \"F8\", \"FT8\", \"FC6\", \"FC4\",\n",
    "            \"FC2\", \"FCz\", \"Cz\", \"C2\", \"C4\", \"C6\", \"T8\", \"TP8\", \"CP6\", \"CP4\", \"CP2\",\n",
    "            \"P2\", \"P4\", \"P6\", \"P8\", \"P10\", \"PO8\", \"PO4\", \"O2\",\n",
    "        ]\n",
    "        # fmt: on\n",
    "        emg_ch_names = [\"EMG1\", \"EMG2\", \"EMG3\", \"EMG4\"]\n",
    "        ch_names = eeg_ch_names + emg_ch_names \n",
    "        ch_types = [\"eeg\"] * 64 + [\"emg\"] * 4 \n",
    "        montage = make_standard_montage(\"standard_1005\")\n",
    "        resting = data.rest - data.rest.mean(axis=1, keepdims=True)\n",
    "        \n",
    "        eeg_rest = resting * 1e-6\n",
    "\n",
    "        info = create_info(ch_names=ch_names, ch_types=ch_types, sfreq=data.srate)\n",
    "        raw = RawArray(data=eeg_rest, info=info, verbose=False)\n",
    "        raw.set_montage(montage)\n",
    "\n",
    "        return {\"session_0\": {\"run_0\": raw}}\n",
    "\n",
    "    def data_path(\n",
    "        self, subject, path=None, force_update=False, update_path=None, verbose=None\n",
    "    ):\n",
    "        if subject not in self.subject_list:\n",
    "            raise (ValueError(\"Invalid subject number\"))\n",
    "\n",
    "        url = \"{:s}s{:02d}.mat\".format(GIGA_URL, subject)\n",
    "        return dl.data_dl(url, \"GIGADB\", path, force_update, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_r = Cho2017_Rest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = ds_r.get_data([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'session_0': {'run_0': <RawArray | 68 x 34048 (66.5 s), ~17.8 MB, data loaded>}}}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fetch_and_unpack_moabb_data(dataset, subject_ids):\n",
    "    data = dataset.get_data(subject_ids)\n",
    "    raws, subject_ids, session_ids, run_ids = [], [], [], []\n",
    "    for subj_id, subj_data in data.items():\n",
    "        for sess_id, sess_data in subj_data.items():\n",
    "            for run_id, raw in sess_data.items():\n",
    "                raws.append(raw)\n",
    "                subject_ids.append(subj_id)\n",
    "                session_ids.append(sess_id)\n",
    "                run_ids.append(run_id)\n",
    "    description = pd.DataFrame({\n",
    "        'subject': subject_ids,\n",
    "        'session': session_ids,\n",
    "        'run': run_ids\n",
    "    })\n",
    "    return raws, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws,description = _fetch_and_unpack_moabb_data(ds_r, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<RawArray | 68 x 34048 (66.5 s), ~17.8 MB, data loaded>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, subject            1\n",
      "session    session_0\n",
      "run            run_0\n",
      "Name: 0, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "for i in description.iterrows():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_base_ds = [BD(raw, row)\n",
    "                       for raw, (_, row) in zip(raws, description.iterrows())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw, (_, row) in zip(raws, description.iterrows()):\n",
    "    db = BD(raw, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.base.BaseDataset at 0x299101880>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<braindecode.datasets.base.BaseDataset at 0x299e19970>]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_base_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOABBDataset_Rest(BaseConcatDataset):\n",
    "    def __init__(self, dataset, subject_ids, dataset_kwargs=None):\n",
    "        raws, description = _fetch_and_unpack_moabb_data(dataset, subject_ids)\n",
    "        all_base_ds = [BD(raw, row)\n",
    "                       for raw, (_, row) in zip(raws, description.iterrows())]\n",
    "        super().__init__(all_base_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MOABBDataset_Rest(dataset=Cho2017_Rest(), subject_ids=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.base.BaseDataset at 0x2b7f79d90>"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne import Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfreq = dataset.datasets[0].raw.info[\"sfreq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Annotations | 0 segments>"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.datasets[0].raw.annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.preprocessing.windowers import _compute_window_inds, _check_windowing_arguments, WindowsDataset\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset.datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_windows_from_events(\n",
    "        ds, infer_window_size_stride,\n",
    "        trial_start_offset_samples, trial_stop_offset_samples,\n",
    "        window_size_samples=None, window_stride_samples=None,\n",
    "        drop_last_window=False, preload=False,\n",
    "        drop_bad_windows=True, picks=None, reject=None, flat=None,\n",
    "        on_missing='error', accepted_bads_ratio=0.0):\n",
    "\n",
    "    #trial_start_offset_samples = 0 #\n",
    "    #trial_stop_offset_samples = 0 #\n",
    "    #drop_last_window = False #\n",
    "    #accepted_bads_ratio=0.0 #\n",
    "    events_id = None #\n",
    "    #drop_bad_windows = False #\n",
    "\n",
    "    duration = int(ds.raw.n_times/ds.raw.info[\"sfreq\"])\n",
    "    onsets = np.array([0])\n",
    "    stops = onsets+np.array([int(duration*ds.raw.info[\"sfreq\"])])\n",
    "\n",
    "    last_samp = ds.raw.first_samp + ds.raw.n_times\n",
    "    if stops[-1] + trial_stop_offset_samples > last_samp:\n",
    "        raise ValueError(\n",
    "            '\"trial_stop_offset_samples\" too large. Stop of last trial '\n",
    "            f'({stops[-1]}) + \"trial_stop_offset_samples\" '\n",
    "            f'({trial_stop_offset_samples}) must be smaller than length of'\n",
    "            f' recording ({len(ds)}).')\n",
    "\n",
    "    window_size_samples = stops[0] + trial_stop_offset_samples - (onsets[0] + trial_start_offset_samples)\n",
    "    window_stride_samples = window_size_samples\n",
    "\n",
    "    i_trials, i_window_in_trials, starts, stops = _compute_window_inds(onsets, stops, trial_start_offset_samples,\n",
    "        trial_stop_offset_samples, window_size_samples, window_stride_samples, drop_last_window,\n",
    "        accepted_bads_ratio)\n",
    "        \n",
    "    description = -1\n",
    "    events = [[start, window_size_samples, description]\n",
    "                for i_start, start in enumerate(starts)]\n",
    "\n",
    "    events = np.array(events)\n",
    "\n",
    "    description = events[:, -1]\n",
    "\n",
    "    metadata = pd.DataFrame({\n",
    "        'i_window_in_trial': i_window_in_trials,\n",
    "        'i_start_in_trial': starts,\n",
    "        'i_stop_in_trial': stops,\n",
    "        'target': description})\n",
    "\n",
    "    mne_epochs = mne.Epochs(\n",
    "        dataset.datasets[0].raw, events, events_id, baseline=None, tmin=0,\n",
    "        tmax=(window_size_samples - 1) / ds.raw.info[\"sfreq\"],\n",
    "        metadata=metadata,)\n",
    "\n",
    "    if drop_bad_windows:\n",
    "            mne_epochs.drop_bad()\n",
    "\n",
    "    windows_ds = WindowsDataset(mne_epochs, ds.description)\n",
    "    return windows_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows_from_events(\n",
    "        concat_ds, trial_start_offset_samples=0, trial_stop_offset_samples=0,\n",
    "        window_size_samples=None, window_stride_samples=None,\n",
    "        drop_last_window=False, preload=False,\n",
    "        drop_bad_windows=True, picks=None, reject=None, flat=None,\n",
    "        on_missing='error', accepted_bads_ratio=0.0, n_jobs=1):\n",
    "\n",
    "    _check_windowing_arguments(\n",
    "        trial_start_offset_samples, trial_stop_offset_samples,\n",
    "        window_size_samples, window_stride_samples)\n",
    "\n",
    "    infer_window_size_stride = window_size_samples is None\n",
    "\n",
    "    list_of_windows_ds = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(_create_windows_from_events)(\n",
    "            ds, infer_window_size_stride,\n",
    "            trial_start_offset_samples, trial_stop_offset_samples,\n",
    "            window_size_samples, window_stride_samples, drop_last_window,\n",
    "            preload, drop_bad_windows, picks, reject, flat,\n",
    "            on_missing, accepted_bads_ratio) for ds in concat_ds.datasets)\n",
    "\n",
    "    return BaseConcatDataset(list_of_windows_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 4 columns\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1 events and 33792 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "trials = create_windows_from_events(dataset, trial_start_offset_samples=0, trial_stop_offset_samples=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Cho2017 resting state class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MI_prediction.Datasets.Moabb import MOABBDataset_Rest\n",
    "from MI_prediction.Datasets import Cho2017_Rest\n",
    "from MI_prediction.Utils.Windowers import create_windows_from_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MOABBDataset_Rest(dataset=Cho2017_Rest(), subject_ids=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 4 columns\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1 events and 33792 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "sfreq = dataset.datasets[0].raw.info[\"sfreq\"]\n",
    "trials= create_windows_from_events(dataset,trial_start_offset_samples=int(0*sfreq),\n",
    "                    trial_stop_offset_samples=int(0*sfreq), preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 33792)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('MI_pred')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6629aad805afa7a6a59ced7c9bb143c65b3c10657ba34c3536df57a876399260"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
