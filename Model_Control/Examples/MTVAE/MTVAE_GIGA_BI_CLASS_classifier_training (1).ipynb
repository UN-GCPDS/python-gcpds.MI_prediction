{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIpjd4TPdQoP"
      },
      "source": [
        "##INSTALAMOS REPOSITORIO Y LIBRERIAS NECESARIAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-COtVXQfgQ5"
      },
      "outputs": [],
      "source": [
        "from scipy.signal import resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEKEMqh_fD_u"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install braindecode===0.7 ### seteamos la versión de braindecode que es con la que trabajamos en el repositorio que se carga posteriormente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7pYJ2Y7dMVC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from google.colab import drive ## conectamos con onedrive\n",
        "drive.mount('/content/drive')\n",
        "### INSTALAMOS LIBRERIAS NECESARIAS\n",
        "!pip install --upgrade tensorflow\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\"\"\"\n",
        "cargamos el repositorio que es el encargado de gestionar:\n",
        "\n",
        "-  carga de la base de datos\n",
        "- preprocesamiento\n",
        "- segmentación por runs\n",
        "- carga del modelo de MTVAE\n",
        "- entrenamiento\n",
        "- gestión del metodo de validación\n",
        "\n",
        "\"\"\"\n",
        "%pip install -e git+https://github.com/UN-GCPDS/python-gcpds.MI_prediction.git#egg=MI_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX0NRSwYdkfO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "para cargar las clases si esta desde colab cambie en los archivos el nombre de la carpeta asociada al repo por\n",
        "prediction es algo que no corregi por el momento, o cambie el import para el nombre de la carpeta\n",
        "\"\"\"\n",
        "from src.prediction.Model_Control.Global_Class.Global_Class import ModelControl\n",
        "from src.prediction.Model_Control.Global_Class.Global_Class import DatasetControl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amHWj8SZLAO0"
      },
      "outputs": [],
      "source": [
        "### CARGAMOS LOS SUJETOS ESPECIFICOS\n",
        "subjects = [1,2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
        " 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 50, 51, 52]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSiOaz5IfXe0"
      },
      "source": [
        "##PROBAMOS LOS SUJETOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-axRs5QdvNu"
      },
      "outputs": [],
      "source": [
        "###PROBAMOS LOS SUJETOS\n",
        "acc_subj = []\n",
        "\n",
        "#### OBTENEMOS LOS CALLBACKS NECESARIOS\n",
        "callbacks_names = {'early_stopping_train1':'early_stopping','checkpoint_train1':'checkpoint',\n",
        "                   'early_stopping_train2':'early_stopping','checkpoint_train2':'checkpoint',\n",
        "                   'early_stopping_train3':'early_stopping','checkpoint_train3':'checkpoint',\n",
        "                   'early_stopping_train4':'early_stopping','checkpoint_train4':'checkpoint'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAWyTXo7gMV1"
      },
      "outputs": [],
      "source": [
        "for sbj in subjects:\n",
        "\n",
        "\n",
        "    datasetControl = DatasetControl(DatasetName='GIGA') ### DEFINIMOS LA BASE DE DATOS\n",
        "      ### OBTENEMOS LAS SESIONES Y RUNS DE LA BASE DE DATOS SELECCIONADA\n",
        "      sessionsRuns = datasetControl.getSessionsRuns()\n",
        "      X , y , sfreq = datasetControl.LoadDataset(subject = sbj,Sessions_Runs=sessionsRuns) ## SIN SEGMENTAR POR SESIONES\n",
        "      print(\"===============================\")\n",
        "      print(X.shape)\n",
        "      print(y.shape)\n",
        "      print(\"===============================\")\n",
        "      Acc_runs = []\n",
        "\n",
        "      # # ### GIGA TIENE 5 SESIONES USANDO EL MISMO PREPROCESO QUE MTVAE\n",
        "      X_train_run_0,X_train_run_1,X_train_run_2,X_train_run_3,X_train_run_4 = X[0,0,:,:,:,:],X[0,1,:,:,:,:],X[0,2,:,:,:,:],X[0,3,:,:,:,:],X[0,4,:,:,:,:]\n",
        "\n",
        "      y_train_run_0 =  y[0,0,:]\n",
        "      y_train_run_1 =  y[0,1,:]\n",
        "      y_train_run_2 =  y[0,2,:]\n",
        "      y_train_run_3 =  y[0,3,:]\n",
        "      y_train_run_4 =  y[0,4,:]\n",
        "      #y_train_run_5 =  y[0,5,:]\n",
        "\n",
        "      # # ### APLICAMOS RESAMPLE PARA DEFINIR LA NUEVA FRECUENCIA DE MUESTREO QUE ES LA QUE TRABAJAMOS CON EL MODELO DE EEGNET\n",
        "      fs_new = 128\n",
        "      X_train_run_0 = resample(X_train_run_0,int((X_train_run_0.shape[-2]/sfreq)*fs_new),axis=-2)\n",
        "      X_train_run_1 = resample(X_train_run_1,int((X_train_run_1.shape[-2]/sfreq)*fs_new),axis=-2)\n",
        "      X_train_run_2 = resample(X_train_run_2,int((X_train_run_2.shape[-2]/sfreq)*fs_new),axis=-2)\n",
        "      X_train_run_3 = resample(X_train_run_3,int((X_train_run_3.shape[-2]/sfreq)*fs_new),axis=-2)\n",
        "      X_train_run_4 = resample(X_train_run_4,int((X_train_run_4.shape[-2]/sfreq)*fs_new),axis=-2)\n",
        "      #X_train_run_5 = resample(X_train_run_5,int((X_train_run_5.shape[-2]/sfreq)*fs_new),axis=-2)\n",
        "\n",
        "\n",
        "      list_train = np.concatenate([X_train_run_0,X_train_run_1,X_train_run_2,X_train_run_3,X_train_run_4])\n",
        "\n",
        "      ### definimos el número de clases\n",
        "      num_classes = 2\n",
        "\n",
        "\n",
        "      list_y_train = np.concatenate([y_train_run_0,y_train_run_1,y_train_run_2,y_train_run_3,y_train_run_4])\n",
        "\n",
        "\n",
        "      # # ##LISTA PARA GUARDAR EL RENDIMIENTO POR RUN\n",
        "\n",
        "\n",
        "      print(\"==============================\")\n",
        "      print(\"==========\"+str(sbj)+\"=============\")\n",
        "      print(\"==============================\")\n",
        "      # ### CARGAMOS LA BASE DE DATOS BCI2A Y EEGNET\n",
        "      model_args = {'nb_classes':2,'Chans':list_train.shape[1],'Samples':list_train.shape[2],'dropoutRate':0.5}\n",
        "      modelControl = ModelControl(Model = 'MTVAE_with_loss',parameters = model_args )\n",
        "\n",
        "      # ###COMPILAMOS EL MODELO\n",
        "      ### la función de costo de la KL esta definida interna en el modelo agregamos para completar el entreno el mse y categorical_crossentropy\n",
        "      modelControl.compileModel(optimizer='Adam',lr=0.001,loss_list=['mse','categorical_crossentropy'],metrics = ['accuracy'])\n",
        "\n",
        "      # ## EXPERIMENTO\n",
        "      print(list_train.shape,list_y_train.shape)\n",
        "      # definimos el metodo de validación\n",
        "      validation = 'lawhern2018'\n",
        "      # ## '/content/drive/MyDrive/EEG_DATA/EEGNET_RESULTS/EEGNET_ALL_RUNS_BI_GIGA/'+\n",
        "      call_args = [\n",
        "          {'monitor':'val_out_activation_accuracy','patience':100,'min_delta':0.001,'mode':'max','verbose':1,'restore_best_weights':False},\n",
        "          {'filepath':'/content/drive/MyDrive/EEG_DATA/MTVAE_GIGA/'+'checkpoint1_'+str(sbj),'save_format':'tf','monitor':'val_out_activation_accuracy','verbose':1,'save_best_only':True,'save_weights_only':True},\n",
        "          {'monitor':'val_out_activation_accuracy','patience':100,'min_delta':0.001,'mode':'max','verbose':1,'restore_best_weights':False},\n",
        "          {'filepath':'/content/drive/MyDrive/EEG_DATA//MTVAE_GIGA/'+'checkpoint2_'+str(sbj),'save_format':'tf','monitor':'val_out_activation_accuracy','verbose':1,'save_best_only':True,'save_weights_only':True},\n",
        "          {'monitor':'val_out_activation_accuracy','patience':100,'min_delta':0.001,'mode':'max','verbose':1,'restore_best_weights':False},\n",
        "          {'filepath':'/content/drive/MyDrive/EEG_DATA//MTVAE_GIGA/'+'checkpoint3_'+str(sbj),'save_format':'tf','monitor':'val_out_activation_accuracy','verbose':1,'save_best_only':True,'save_weights_only':True},\n",
        "          {'monitor':'val_out_activation_accuracy','patience':100,'min_delta':0.001,'mode':'max','verbose':1,'restore_best_weights':False},\n",
        "          {'filepath':'/content/drive/MyDrive/EEG_DATA//MTVAE_GIGA/'+'checkpoint4_'+str(sbj),'save_format':'tf','monitor':'val_out_activation_accuracy','verbose':1,'save_best_only':True,'save_weights_only':True},\n",
        "      ]\n",
        "\n",
        "      History,acc_model  = modelControl.train_model(X_train=list_train,Y_train=[list_train,list_y_train],x_val=list_train,y_val=[list_train,list_y_train],callbacks_names = callbacks_names,call_args =call_args,validation_mode = validation, batch_size =16,epochs = 500,verbose =1,autoencoder=True)\n",
        "\n",
        "      print(\"ACCURACY PER SUBJECT: \",acc_model)\n",
        "      acc_subj.append(acc_model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fKjOOITIAl2"
      },
      "outputs": [],
      "source": [
        "### RESULTADOS OBTENIDOS\n",
        "acc_subj = [89.0,\n",
        " 66.5,\n",
        " 98.0,\n",
        " 90.0,\n",
        " 99.0,\n",
        " 84.0,\n",
        " 72.0,\n",
        " 79.5,\n",
        " 77.5,\n",
        " 90.5,\n",
        " 73.5,\n",
        " 75.5,\n",
        " 96.5,\n",
        " 98.0,\n",
        " 83.5,\n",
        " 76.5,\n",
        " 78.5,\n",
        " 78.0,\n",
        " 80.5,\n",
        " 75.0,\n",
        " 63.5,\n",
        " 82.0,\n",
        " 95.5,\n",
        " 78.0,\n",
        " 87.0,\n",
        " 96.5,\n",
        " 78.5,\n",
        " 90.0,\n",
        " 81.0,\n",
        " 78.0,\n",
        " 79.0,\n",
        " 79.0,\n",
        " 70.5,\n",
        " 85.5,\n",
        " 78.0,\n",
        " 87.0,\n",
        " 66.5,\n",
        " 80.0,\n",
        " 77.5,\n",
        " 96.0,\n",
        " 79.5,\n",
        " 94.0,\n",
        " 95.0,\n",
        " 75.0,\n",
        " 83.0,\n",
        " 96.0,\n",
        " 100.0,\n",
        " 76.5,\n",
        " 78.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scf3bd0ztFrx"
      },
      "outputs": [],
      "source": [
        "len(acc_subj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBBxXUNntH5Y"
      },
      "outputs": [],
      "source": [
        "len(subjects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW6ZEuEAtNQT"
      },
      "source": [
        "##MIRAMOS POR RUNS PARA CADA SUJETO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9n7LbKUvPmR"
      },
      "outputs": [],
      "source": [
        "Acc__subs_runs = []\n",
        "# subjects=[43]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2hAmIDotPT4"
      },
      "outputs": [],
      "source": [
        "for sbj in [51,52]:\n",
        "\n",
        "    datasetControl = DatasetControl(DatasetName='GIGA') ### DEFINIMOS LA BASE DE DATOS\n",
        "    ### OBTENEMOS LAS SESIONES Y RUNS DE LA BASE DE DATOS SELECCIONADA\n",
        "    sessionsRuns = datasetControl.getSessionsRuns()\n",
        "    X , y , sfreq = datasetControl.LoadDataset(subject = sbj,Sessions_Runs=sessionsRuns) ## SIN SEGMENTAR POR SESIONES\n",
        "\n",
        "    Acc_runs = []\n",
        "    # ### BCI2A TIENE 2 SESIONES USANDO EL MISMO PREPROCESO QUE MTVAE\n",
        "    X_train_run_0,X_train_run_1,X_train_run_2,X_train_run_3,X_train_run_4 = X[0,0,:,:,:,:],X[0,1,:,:,:,:],X[0,2,:,:,:,:],X[0,3,:,:,:,:],X[0,4,:,:,:,:]\n",
        "\n",
        "    y_train_run_0 =  y[0,0,:]\n",
        "    y_train_run_1 =  y[0,1,:]\n",
        "    y_train_run_2 =  y[0,2,:]\n",
        "    y_train_run_3 =  y[0,3,:]\n",
        "    y_train_run_4 =  y[0,4,:]\n",
        "    #y_train_run_5 =  y[0,5,:]\n",
        "\n",
        "\n",
        "    # ### APLICAMOS RESAMPLE\n",
        "    fs_new = 128\n",
        "    X_train_run_0 = resample(X_train_run_0,int((X_train_run_0.shape[-2]/sfreq)*fs_new),axis=-2)\n",
        "    X_train_run_1 = resample(X_train_run_1,int((X_train_run_1.shape[-2]/sfreq)*fs_new),axis=-2)\n",
        "    X_train_run_2 = resample(X_train_run_2,int((X_train_run_2.shape[-2]/sfreq)*fs_new),axis=-2)\n",
        "    X_train_run_3 = resample(X_train_run_3,int((X_train_run_3.shape[-2]/sfreq)*fs_new),axis=-2)\n",
        "    X_train_run_4 = resample(X_train_run_4,int((X_train_run_4.shape[-2]/sfreq)*fs_new),axis=-2)\n",
        "    #X_train_run_5 = resample(X_train_run_5,int((X_train_run_5.shape[-2]/sfreq)*fs_new),axis=-2)\n",
        "\n",
        "\n",
        "    list_train = [X_train_run_0,X_train_run_1,X_train_run_2,X_train_run_3,X_train_run_4]\n",
        "\n",
        "    num_classes = 2\n",
        "\n",
        "\n",
        "    list_y_train = [y_train_run_0,y_train_run_1,y_train_run_2,y_train_run_3,y_train_run_4]\n",
        "\n",
        "\n",
        "    # ##LISTA PARA GUARDAR EL RENDIMIENTO POR RUN\n",
        "\n",
        "    for i in range(0,len(list_train)):\n",
        "\n",
        "        print(\"==============================\")\n",
        "        print(\"==========\"+str(sbj)+'RUN'+str(i+1)+\"=============\")\n",
        "        print(\"==============================\")\n",
        "        ### CARGAMOS LA BASE DE DATOS BCI2A Y EEGNET\n",
        "        model_args = {'nb_classes':2,'Chans':list_train[i].shape[1],'Samples':list_train[i].shape[2],'dropoutRate':0.5}\n",
        "        modelControl = ModelControl(Model = 'MTVAE_with_loss',parameters = model_args )\n",
        "\n",
        "        # ###COMPILAMOS EL MODELO\n",
        "        modelControl.compileModel(optimizer='Adam',lr=0.001,loss_list=['mse','categorical_crossentropy'],metrics = ['accuracy'])\n",
        "\n",
        "        ## EXPERIMENTO\n",
        "\n",
        "        validation = 'lawhern2018'\n",
        "        ## '/content/drive/MyDrive/EEG_DATA/EEGNET_RESULTS/EEGNET_ALL_RUNS_BI_GIGA/'+\n",
        "        call_args = [\n",
        "         {'monitor':'val_out_activation_accuracy','patience':100,'min_delta':0.001,'mode':'max','verbose':1,'restore_best_weights':False},\n",
        "         {'filepath':'/content/drive/MyDrive/EEG_DATA/MTVAE_GIGA/sbj_'+str(sbj)+'_Run_'+str(i+1)+'_'+'checkpoint1','save_format':'tf','monitor':'val_out_activation_accuracy','verbose':1,'save_best_only':True,'save_weights_only':True},\n",
        "         {'monitor':'val_out_activation_accuracy','patience':100,'min_delta':0.001,'mode':'max','verbose':1,'restore_best_weights':False},\n",
        "         {'filepath':'/content/drive/MyDrive/EEG_DATA/MTVAE_GIGA/sbj_'+str(sbj)+'_Run_'+str(i+1)+'_'+'checkpoint2','save_format':'tf','monitor':'val_out_activation_accuracy','verbose':1,'save_best_only':True,'save_weights_only':True},\n",
        "         {'monitor':'val_out_activation_accuracy','patience':100,'min_delta':0.001,'mode':'max','verbose':1,'restore_best_weights':False},\n",
        "         {'filepath':'/content/drive/MyDrive/EEG_DATA/MTVAE_GIGA/sbj_'+str(sbj)+'_Run_'+str(i+1)+'_'+'checkpoint3','save_format':'tf','monitor':'val_out_activation_accuracy','verbose':1,'save_best_only':True,'save_weights_only':True},\n",
        "         {'monitor':'val_out_activation_accuracy','patience':100,'min_delta':0.001,'mode':'max','verbose':1,'restore_best_weights':False},\n",
        "         {'filepath':'/content/drive/MyDrive/EEG_DATA/MTVAE_GIGA/sbj_'+str(sbj)+'_Run_'+str(i+1)+'_'+'checkpoint4','save_format':'tf','monitor':'val_out_activation_accuracy','verbose':1,'save_best_only':True,'save_weights_only':True},\n",
        "        ]\n",
        "\n",
        "        History,acc_model  = modelControl.train_model(X_train=list_train[i],Y_train=[list_train[i],list_y_train[i]],x_val=list_train[i],y_val=[list_train[i],list_y_train[i]],callbacks_names = callbacks_names,call_args =call_args,validation_mode = validation, batch_size =16,epochs = 500,verbose =1,autoencoder=True)\n",
        "\n",
        "        Acc_runs.append(acc_model)\n",
        "    print(\"////////////////////////////\")\n",
        "    print(\"////////////////////////////\")\n",
        "    print(\"SUBJECT RUNS: \",Acc_runs)\n",
        "    print(\"////////////////////////////\")\n",
        "    print(\"////////////////////////////\")\n",
        "    Acc__subs_runs.append(Acc_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz6Pvf6WCNF6"
      },
      "outputs": [],
      "source": [
        "Acc__subs_runs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}